# PYMECompress 使用指南

## 安装

### 方式一：conda安装（推荐）

```bash
conda install -c david_baddeley pymecompress
```

**优点**: 预编译二进制，无需编译环境

### 方式二：pip安装

```bash
pip install pymecompress
```

**注意**: 部分平台可能需要编译，需要准备编译环境

### 方式三：源码安装

#### Linux/macOS

```bash
git clone https://github.com/python-microscopy/pymecompress.git
cd pymecompress
pip install -r requirements.txt
python setup.py install
```

或开发模式：
```bash
python setup.py develop
```

#### Windows

Windows需要MinGW编译器：

```bash
# 1. 创建conda环境
conda create -n pymecompress python=3.8 numpy cython libpython m2w64-toolchain

# 2. 激活环境
conda activate pymecompress

# 3. 编译安装
python setup.py build --compiler=mingw32
python setup.py install
```

## 基本使用

### 方式一：直接调用函数（低级API）

适用于需要完全控制压缩过程的场景。

#### 无损压缩

```python
import numpy as np
import pymecompress

# 准备数据（必须是uint8视图）
data = np.random.randint(0, 255, 10000, dtype='uint16')
data_bytes = np.frombuffer(data.data, dtype='uint8')

# 压缩
compressed = pymecompress.HuffmanCompress(data_bytes)

# 解压缩（需要提供原始大小）
nbytes = data.nbytes
decompressed = pymecompress.HuffmanDecompress(
    np.frombuffer(compressed, 'u1'), 
    nbytes
)

# 恢复原始数据类型
result = decompressed.view('uint16')
```

#### 带量化的压缩（用于uint16图像）

```python
import numpy as np
import pymecompress

# 模拟光子计数图像
data = np.random.poisson(100, (512, 512)).astype('uint16')

# 量化参数
quantization_offset = 0  # 背景偏移
quantization_scale = 1.0  # 缩放因子

# 压缩（自动量化）
compressed = pymecompress.HuffmanCompressQuant(
    data.ravel(), 
    quantization_offset, 
    quantization_scale
)

# 解压缩
nbytes_quantized = data.size  # 注意：量化后是uint8
decompressed = pymecompress.HuffmanDecompress(
    np.frombuffer(compressed.to_string(), 'u1'),
    nbytes_quantized
)

# 反量化
dequantized = (quantization_scale * decompressed.astype('float')) ** 2 + quantization_offset
result = dequantized.reshape(data.shape).astype('uint16')

# 验证误差在泊松噪声范围内
error = np.abs(result.astype('float') - data.astype('float'))
noise = np.sqrt(data.astype('float'))
print(f"最大信噪比: {np.max(error / noise):.2f}")  # 应该 < 1
```

### 方式二：使用numcodecs编解码器（推荐）

更简单的API，自动处理大小和类型。

#### 无损霍夫曼编码

```python
import numpy as np
from pymecompress import codecs

# 创建编解码器
huff = codecs.Huffman()

# 压缩（输入必须是uint8）
data = np.ones(1000, dtype='float64')
compressed = huff.encode(data.view('uint8'))

# 解压缩（自动恢复大小）
decompressed = huff.decode(compressed)

# 恢复原始类型
result = decompressed.view(data.dtype)
assert np.allclose(result, data)
```

#### 带量化的编解码器

```python
import numpy as np
from pymecompress import codecs

# 创建量化编解码器
huffq = codecs.HuffmanQuant16(offset=0, scale=1.0)

# 准备数据（必须是uint16）
data = np.linspace(1, 2**15, 10000).astype('uint16')

# 压缩（自动量化）
compressed = huffq.encode(data)

# 解压缩（自动反量化）
decompressed = huffq.decode(compressed)

# 验证误差
error = decompressed.astype('float') - data.astype('float')
noise = np.sqrt(data.astype('float'))
assert np.all(error < noise)
```

### 方式三：与Zarr集成

最高级的使用方式，适合大规模数据存储。

#### 基本使用

```python
import zarr
import numpy as np
from pymecompress import codecs

# 创建带压缩的Zarr数组
z = zarr.zeros(
    (1000, 1000), 
    dtype='uint16',
    chunks=(100, 100),
    compressor=codecs.HuffmanQuant16(offset=0, scale=1)
)

# 写入数据
z[:] = np.random.poisson(100, (1000, 1000))

# 保存到磁盘
zarr.save('mydata.zarr', z)
```

#### 读取已压缩的数据

```python
import zarr
from pymecompress import codecs  # 必须导入以注册编解码器

# 读取
z = zarr.open('mydata.zarr', mode='r')
data = z[:]
```

**重要**: 读取pymecompress压缩的数据前，必须先 `from pymecompress import codecs` 以注册编解码器。

## 高级用法

### 选择合适的量化参数

量化参数决定压缩率和精度的平衡。

```python
import numpy as np
from pymecompress import codecs

# 准备测试数据
data = np.random.poisson(100, 100000).astype('uint16')

# 测试不同的scale参数
for scale in [0.5, 1.0, 2.0, 5.0]:
    codec = codecs.HuffmanQuant16(offset=0, scale=scale)
    compressed = codec.encode(data)
    decompressed = codec.decode(compressed)
    
    compression_ratio = data.nbytes / len(compressed)
    rmse = np.sqrt(np.mean((data.astype('float') - decompressed.astype('float'))**2))
    
    print(f"Scale={scale}: 压缩比={compression_ratio:.2f}x, RMSE={rmse:.2f}")
```

**经验规则**:
- `scale=1.0`: 标准设置，适合大多数光子计数图像
- `scale<1.0`: 更高精度，较低压缩率
- `scale>1.0`: 更高压缩率，较低精度

### 处理背景偏移

对于有固定背景的图像：

```python
# 假设图像有100个计数的背景
background = 100
data = np.random.poisson(100, 10000).astype('uint16') + background

# 设置offset移除背景
codec = codecs.HuffmanQuant16(offset=background, scale=1.0)
compressed = codec.encode(data)
decompressed = codec.decode(compressed)
```

### 批量处理多帧图像

```python
import numpy as np
from pymecompress import codecs

# 多帧图像 (时间, 高度, 宽度)
frames = np.random.poisson(100, (100, 512, 512)).astype('uint16')

codec = codecs.HuffmanQuant16(offset=0, scale=1.0)

# 逐帧压缩
compressed_frames = []
for frame in frames:
    compressed_frames.append(codec.encode(frame.ravel()))

# 计算总压缩比
original_size = frames.nbytes
compressed_size = sum(len(c) for c in compressed_frames)
print(f"总压缩比: {original_size / compressed_size:.2f}x")
```

### 与HDF5结合

```python
import h5py
import numpy as np
from pymecompress import codecs

codec = codecs.HuffmanQuant16(offset=0, scale=1.0)
data = np.random.poisson(100, (1000, 1000)).astype('uint16')

# 压缩并存储
compressed = codec.encode(data.ravel())

with h5py.File('data.h5', 'w') as f:
    f.create_dataset('compressed', data=np.frombuffer(compressed, 'uint8'))
    f.attrs['original_shape'] = data.shape
    f.attrs['offset'] = 0
    f.attrs['scale'] = 1.0

# 读取并解压
with h5py.File('data.h5', 'r') as f:
    compressed = bytes(f['compressed'][:])
    shape = f.attrs['original_shape']
    offset = f.attrs['offset']
    scale = f.attrs['scale']
    
    codec = codecs.HuffmanQuant16(offset=offset, scale=scale)
    decompressed = codec.decode(compressed).reshape(shape)
```

## 性能优化技巧

### 1. 使用连续内存

```python
# 不好 - 非连续内存
data = np.random.rand(1000, 1000)[::2, ::2].astype('uint16')

# 好 - 连续内存
data = np.ascontiguousarray(data)
compressed = codec.encode(data.ravel())
```

### 2. 预分配输出缓冲区

```python
# 对于大量解压缩操作
output_buffer = np.empty(expected_size, dtype='uint16')
decompressed = codec.decode(compressed, out=output_buffer)
```

### 3. 多线程处理

```python
from concurrent.futures import ThreadPoolExecutor
from pymecompress import codecs

def compress_frame(frame):
    codec = codecs.HuffmanQuant16(offset=0, scale=1.0)
    return codec.encode(frame.ravel())

frames = [np.random.poisson(100, (512, 512)).astype('uint16') for _ in range(100)]

with ThreadPoolExecutor(max_workers=4) as executor:
    compressed = list(executor.map(compress_frame, frames))
```

## 常见问题

### Q1: 压缩时出现"buffer is not contiguous"错误

**原因**: 数组内存不连续

**解决**:
```python
data = np.ascontiguousarray(data)
```

### Q2: 解压后数据类型不对

**原因**: 忘记转换类型

**解决**:
```python
decompressed = codec.decode(compressed).astype('uint16')
```

### Q3: Zarr读取失败"codec not available"

**原因**: 未注册编解码器

**解决**:
```python
from pymecompress import codecs  # 必须在读取前导入
z = zarr.open('data.zarr')
```

### Q4: 量化误差过大

**原因**: scale参数不合适

**解决**: 减小scale值，例如从1.0改为0.5

### Q5: Windows编译失败

**原因**: 缺少MinGW编译器

**解决**:
```bash
conda install m2w64-toolchain
python setup.py build --compiler=mingw32
```

## 性能基准

在Intel Core i7处理器上的典型性能：

| 操作 | 数据类型 | 吞吐量 | 压缩比 |
|------|---------|-------|--------|
| 霍夫曼压缩 | uint8 | ~600 MB/s | 1.5-2x |
| 霍夫曼解压 | uint8 | ~550 MB/s | - |
| 量化+压缩 | uint16 | ~500 MB/s | 3-5x |
| 解压+反量化 | uint16 | ~480 MB/s | - |

**注意**: 实际性能取决于数据特性和硬件配置。

## 最佳实践

1. **选择合适的API级别**:
   - 简单场景：使用numcodecs
   - 与Zarr集成：使用编解码器
   - 需要精细控制：使用低级API

2. **内存管理**:
   - 确保数组连续
   - 大数据集使用分块处理
   - 考虑使用内存映射

3. **参数调优**:
   - 根据数据特性选择offset
   - 通过实验确定最佳scale
   - 平衡压缩比和精度

4. **错误处理**:
   - 检查输入数据类型
   - 验证压缩/解压缩结果
   - 处理可能的内存错误

5. **并行化**:
   - 多帧独立处理
   - 使用线程池
   - 注意GIL限制（大部分时间在C代码中）

